# -*- coding: utf-8 -*-
"""CustomerComplain-NLP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j1kRVfiLttQmC-jmAP3MExOBHg2DKZZM

# Proyek Pertama Machine Learning Intermediate: Membuat Model NLP dengan Tensorflow
- Nama: Indriyati Rahmi Setyani
- Email: indriyatirs@gmail.com
- Id Dicoding: indriyatirs
- Dataset: Customer Complaints Dataset for NLP
- Sumber: Kaggle

## Import & Understanding Data
"""

import pandas as pd
import numpy as np

import zipfile, os

!gdown 1QsGlxAln4tgq0n3nmZJcD-Jrdxatwlqk

! unzip /content/customercomplain.zip

df = pd.read_csv('complaints_processed.csv')

df.head()

df.isnull().sum()

"""Dataset terdiri dari 3 kolom yaitu kolom Unnamed: 0, product dan narrative. Dari ketiga kolom tersebut terdapat 10 baris yang mengandung data null pada kolom narrative. Data null ini akan dihilangkan. Kemudian kolom yang akan digunakan pada proses NLP hanya kolom product dan narrative."""

df = df.dropna(subset=['narrative'])

df = df.drop(columns=['Unnamed: 0'])

df.shape

df.describe()

df['product'].value_counts()

"""Dataframe yang digunakan memiliki sebanyak 162411 baris dan 2 kolom. Kolom product memiliki 5 kategori dan kategori yang paling sering muncul adalah credit_reporting sebanyak 91172 kali."""

# menampilkan salah satu teks plot dari film
print(df[df.index == 34881]['narrative'].values[0])

"""## Membuat Data Label"""

category = pd.get_dummies(df['product'])
new_df = pd.concat([df, category],axis=1)
new_df = new_df.drop(columns=['product'])
new_df

new_df.shape

"""## Membersihkan Stopwords dan Punctuation(tanda baca)"""

import nltk
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('stopwords')

nltk.download('punkt')

new_df['narrative'] = new_df['narrative'].str.lower()

"""### Menghapus stopwords"""

stop_words = set(stopwords.words('english'))

new_df['desc'] = new_df['narrative'].apply(lambda x: " ".join(x for x in x.split() if x not in stop_words))

new_df

"""### Menghapus punctuation"""

import string

new_df['desc'] = new_df['desc'].str.replace('[{}]'.format(string.punctuation), '', regex=True)

new_df

"""## Melakukan Spliting Data"""

df['product'].unique()

from sklearn.model_selection import train_test_split

desc = new_df['desc'].values
prod = new_df[['credit_card', 'retail_banking', 'credit_reporting',
       'mortgages_and_loans', 'debt_collection']].values

X_train, X_test, y_train, y_test = train_test_split(desc, prod, test_size=0.2) #random_state=42

"""## Melakukan Tokenizer"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

num_words = 2500

tokenizer = Tokenizer(num_words=num_words, oov_token='XX')
tokenizer.fit_on_texts(X_train)

sequences_X_train = tokenizer.texts_to_sequences(X_train)
sequences_X_test = tokenizer.texts_to_sequences(X_test)

input_length = 150

padded_X_train = pad_sequences(sequences_X_train, maxlen=input_length)
padded_X_test = pad_sequences(sequences_X_test, maxlen=input_length)

"""## Arsitektur Model"""

import tensorflow as tf

import tensorflow as tf
import numpy as np
#from tensorflow.keras.models import Sequential
#from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dropout,  Dense
from tensorflow.keras.regularizers import l1_l2, l2

model = tf.keras.models.Sequential([
   tf.keras.layers.Embedding(input_dim=num_words, output_dim=32, input_length=input_length),
   tf.keras.layers.SpatialDropout1D(0.01),
   tf.keras.layers.LSTM(64, kernel_regularizer=l2(0.02), recurrent_regularizer=l2(0.02), bias_regularizer=l2(0.02),
                        recurrent_dropout=0,
                        input_shape=(padded_X_train.shape[0], padded_X_train.shape[1]),
                        return_sequences=True),
   tf.keras.layers.Dropout(0.02),
   tf.keras.layers.LSTM(32),
   #tf.keras.layers.Dense(32),
   tf.keras.layers.Dense(5, activation='softmax')
])

model.summary()

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_accuracy') > 0.87):
      print("\nIterasi berhenti, akurasi model lebih dari 87%")
      self.model.stop_training = True

callbacks = myCallback()

num_epochs = 100
history = model.fit(padded_X_train, y_train,
                    #batch_size=16,
                    epochs=num_epochs,
                    validation_data=(padded_X_test, y_test),
                    verbose=2, callbacks=[callbacks])

print(f"Train Accuracy: {history.history['accuracy'][-1]}")
print(f"Validation Accuracy: {history.history['val_accuracy'][-1]}")

import matplotlib.pyplot as plt

fig, ax = plt.subplots(1, 2, figsize = (10, 4))

ax[0].plot(history.history['loss'])
ax[0].plot(history.history['val_loss'])

ax[1].plot(history.history['accuracy'])
ax[1].plot(history.history['val_accuracy'])

ax[0].set_ylabel('loss')
ax[1].set_ylabel('accuracy')
ax[0].set_xlabel('epoch')
ax[1].set_xlabel('epoch')
ax[0].set_title('Nilai loss pada tiap epoch')
ax[1].set_title('Nilai accuracy pada tiap epoch')
#ax[0].set_xticks(ticks=range(0, 24), labels=range(0,24))
#ax[1].set_xticks(ticks=range(0, 24), labels=range(0,24))

ax[0].legend(['training', 'validation'], loc='upper left')
ax[1].legend(['training', 'validation'], loc='upper left')
plt.show()

"""Nilai akurasi yang didapatkan pada model adalah 87%. Pada grafik loss dan accuracy juga menunjukkan bahwa model sudah cukup baik dan tidak mengalami overfitting."""